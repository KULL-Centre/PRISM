#!/usr/local/bin/python3

# Copyright (C) 2021 Kristoffer Enoe Johansson <kristoffer.johansson@bio.ku.dk>

__version__ = 0.03

from argparse import ArgumentParser
import os,sys,argparse
import urllib
import json
import time

from Bio import SeqIO
from Bio import Align
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
from io import StringIO

import numpy as np
import pandas as pd
pd.set_option('display.max_colwidth', 20)

# Global variables to ensure that servers are not pinged too often
__pdb_time = 0.0
__uniprot_time = 0.0
__sifts_time = 0.0


def select_structures(uniprot_record, struc_map, n_select=1, min_cover_positions=40, identity_power=1.0, verbose=0):
    """Select a set of structures that best represent a uniprot protein based on a structure map

    Assigns a score to each structure and selects the best scoring structure per position:
    score = 2.5/method_score * coverage * identitiy**identity_power
    where method_score is X-ray resolution, 3 for cryo-em, 4 for nmr and 10 for others

    Parameters
    ----------
    uniprot_record : BioPythin SeqRecord object parsed using uniprot-xml format
        Information on target protein
    struc_map : Pandas DateFrame
        Structure map as generated by url_strucmap function below
    identity_power : float
        In the score function, the fraction identity is raised to this power to achieve a steeper 
        decent from identity 1.0. Default 1.0 for linear scaling.
    min_cover_positions : integer
        The minimum number of position that a structure should be optimal for in order to 
        appear in the final selection. Default 40 thought as a minimum size of a folded domain.
    verbose : integer
        Default 0 is silent and higher integers increses level of terminal output
    """
    uniprot_id = uniprot_record.id
    uniprot_seq = str(uniprot_record.seq)
    unp_nres = len(uniprot_seq)
    nstruc = len(struc_map.index)
    res_score_matrix = np.ones([nstruc, unp_nres]) * -1
    score_list = [-1.0]*nstruc
    for i in range(nstruc):
        pdb_str = "%s-%s" % (struc_map['pdb_id'][i], struc_map['chain_id'][i])
        seq = struc_map['aligned_resseq'][i]
        # pdb_nres = struc_map['end'][i] - struc_map['start'][i] + 1

        # # The indentity measure in the score function compeates with coverage because the coverage can be made larger at the expence
        # # of identity. 
        # ident = struc_map['ident'][i] / pdb_nres

        # Count identities among observed residues 
        ident_obs = sum( [aa1==aa2 and not aa2 in "-x" for aa1,aa2 in zip(uniprot_seq.lower(),struc_map['aligned_resseq_obs'][i].lower())] )
        # Number of positions that align, checked to be SIFTS coverage times len(uniprot_seq)
        cover = struc_map['coverage'][i]
        ident_score = ( ident_obs*1.0/cover )**identity_power

        # Number of oberserved positions
        align_obs = struc_map['aligned_resseq_obs'][i].lower()
        cover_obs = (unp_nres - align_obs.count('-') - align_obs.count('x')) / unp_nres
        
        # Assign a score to the method used to determine the structure
        method = (struc_map['experimental_method'][i]).lower()
        if 'x-ray' in method or 'xray' in method:
            method_score = struc_map['resolution'][i]
        elif 'cryoem' in method or 'cryo-em' in method:
            # Cryo-EM structure same as 3Å X-ray
            method_score = 3.0
            # Consider to look for EM resilution
        elif 'nmr' in method:
            # NMR structure same as 4Å X-ray
            method_score = 4.0
        else:
            # Assume model, only prefer when nothing else is available
            method_score = 5.0
            
        # Structure score used for selecting best structure for uniprot
        score_list[i] = 1/(1+np.exp((method_score-3.5)*2)) + cover_obs * ident_score
        # method_score:   1.8,   2.2,   2.5,   2.7,   3.0,   4.0,   5.0
        # sigmoid       0.968, 0.931, 0.881, 0.832, 0.731, 0.269, 0.047
        
        # Structures of identical score is typically different but identical chains of the same asym unit, here argmax selects the first
        if verbose > 0:
            print("%s %s score %.2f (%s, method_score %.2f, cover_obs %.3f, ident_score = %d/%d^%.1f = %.3f)" %
                  (uniprot_id, pdb_str, score_list[i], method, method_score, cover_obs, ident_obs, cover, identity_power, ident_score))
            
        # Assign structure score to positions that structure cover
        seq_array = np.array(list(struc_map['aligned_resseq_obs'][i]))
        pos_mask = np.logical_and(seq_array != '-', seq_array != 'x')
        res_score_matrix[i,pos_mask] = score_list[i]
        
    struc_map['score'] = score_list
    
    # For each position (column in res_score_matrix), find the best (highest scoring) structure
    best_struc_index = np.argmax(res_score_matrix, axis=0)

    # Count how many positions each structure best represent
    best_struc_counts = np.bincount(best_struc_index)

    # Fill array with structure indices that are never optimal and store in structure map
    if best_struc_counts.size < nstruc:
        best_struc_counts = np.append(best_struc_counts, [0]*(nstruc-best_struc_counts.size))
    struc_map['selected_pos'] = best_struc_counts

    # Which structure(s) covers most positions
    struc_rank = np.flip( np.argsort(best_struc_counts) )
    if verbose > 1:
        # print("Matrix with indices of best structure per position")
        # print(res_score_matrix)
        print("Index of best scoring structure per position")
        print(best_struc_index)
        print("Number of positions that each structure (row) is selected for: %s" % (str(best_struc_counts)))
        print("Ranked indices of the structure map raows: %s" % (str(struc_rank)))

    # Do final selection
    selected_indices = []
    for rank in range(len(struc_rank)):
        struc_index = struc_rank[rank]
        if verbose > 0:
            print("Rank %2d is index %2d %s-%s that covers %d positions" %
                  (rank, struc_index, struc_map['pdb_id'][struc_index],struc_map['chain_id'][struc_index], best_struc_counts[struc_index]))
        # Check if structure is ok to use
        if best_struc_counts[struc_index] > min_cover_positions:
            # print("Using %s chain %s" % (struc_map['pdb_id'][struc_index],struc_map['chain_id'][struc_index]))
            selected_indices.append(struc_index)

    # Report
    if verbose > 0:
        nsel = len(selected_indices)
        if nsel == 1:
            i = selected_indices[0]
            print("%s: %s covers %d of %d residues" % (uniprot_id, pdb_str, struc_map['coverage'][i], len(uniprot_seq)))
        elif nsel > 1:
            struc_str = ", ".join( ["%s-%s cover %d" % (pdb,c,cover*100) for (pdb,c,cover) in
                                    zip(struc_map['pdb_id'][selected_indices],
                                        struc_map['chain_id'][selected_indices],
                                        struc_map['coverage'][selected_indices])] )
            print("%s covered by %d structures: %s" % (uniprot_id, nsel, struc_str))

            # Determine overlap of structures
            overlap = np.zeros([nsel,nsel])
            for i in range(nsel-1):
                for j in range(i+1,nsel):
                    overlap[i,j] = sum( np.logical_and(res_score_matrix[i,] > 0, res_score_matrix[j,] > 0) )
            if nsel < 3:
                print("The two structures overlap at %d positions" % (overlap[1,1]))
            else:
                print("Overlaping positions matrix:")
                print(overlap)
        else:
           print("ERROR: No structures found for %s" % (uniprot_id))

    # Return the rows of stuc_map input that was selected
    return(struc_map.iloc[selected_indices,:])


def get_method_score(smap_row, verbose=0):
    method = smap_row.method_exp.lower()
    if 'x-ray' in method or 'xray' in method:
        method_score = smap_row.method_res
        if method_score is None:
            if verbose > 0:
                print("%s-%s: No resolution given for X-ray structure. Using 2.5" % (smap_row.pdb, smap_row.chain))
            method_score = 2.5
    elif 'electron microscopy' in method:
        method_score = smap_row.method_res
        if method_score is None:
            if verbose > 0:
                print(" %s-%s: No resolution given for EM structure. Using 3.5" % (smap_row.pdb, smap_row.chain))
            method_score = 3.5
    elif 'nmr' in method:
        # NMR structure same as 4Å X-ray                                                                                                                 
        method_score = 4.0
    else:
        # Assume model, only prefer when nothing else is available
        if verbose > 0:
            print("%s-%s: Assuming method '%s' is model" % (smap_row.pdb, smap_row.chain, smap_row.method_exp))
        method_score = 5.0
    return(method_score)


def rank_structures(target_record, strucmap, verbose=0):
    nres_target = len(target_record.seq)

    # Coverage score
    # Conceptually, score terms are weighted in relation to coverage because this is available early on from SIFTS
    # Cover_exp includes un-observed residues but not deletions (actually deleted from the molecules)
    # Cover_obs includes mismatch and modified residues but not un-observed
    cover_score = 0.0
    if 'cover_obs' in strucmap.columns:
        cover_score = strucmap['cover_obs']
    elif 'cover_exp' in strucmap.columns:
        cover_score = strucmap['cover_exp']

    # Score to evaluate the experimental method by which the structure was determined
    method_scores = 0.0
    if 'method_exp' in strucmap.columns:
        method_scores = np.array( [get_method_score(row, verbose=verbose) for row in strucmap.itertuples(index=False)] )
        # method_scores    1.3,    1.5,    1.7,    1.9,    2.1,    2.2,    2.3,    2.5,    3.0,    4.0,    5.0
        # sigmoid       0.9955, 0.9933, 0.9900, 0.9852, 0.9781, 0.9734, 0.9677, 0.9526, 0.8808, 0.5000, 0.1192
        method_scores = 1/(1+np.exp((method_scores-3.8)*2))

    # Score based on inserts in the structure (not present in the target sequence), see case P07550
    insert_score = 0
    if 'inserts' in strucmap.columns:
        # Inserts are completely removed from the alignment so they need to be penalized
        insert_score = -5.0 * strucmap['inserts'] / nres_target
        
    # Score based on chemically modified residues 
    mod_score = 0
    if 'modified' in strucmap.columns:
        # If cover and identity is the same, prefer structure with least modified residues
        # If a structure has one additional aligned residue, it counts 1.5/nres, and a modified sould be worse
        # since this is typically in the folded part of the structure whereas the additionl residue likely to
        # be in the terminals
        mod_score = - 1.7 * strucmap['modified'] / nres_target

    # Mismatch score
    mismatch_score = 0
    if 'mismatch' in strucmap.columns:
        # A mismatch should weight more than 2 residue coverage because coverage is typically extended in
        # terminals and a mismatch in the folded region
        mismatch_score = -2.5 * strucmap['mismatch'] / nres_target

    # Everything else equal, prefer chain A
    chain_score = 0.0
    if 'chain' in strucmap.columns:
        chain_score = 1e-5 * (strucmap['chain']=='A')
        
    # Total score
    scores = method_scores + cover_score + insert_score + mod_score + mismatch_score + chain_score
    strucmap['score'] = scores

    # Reorder structure map to have highest scoring structures on top
    strucmap = strucmap.iloc[ np.flip(np.argsort(scores)) ]
    # Rename row to running indices and do not create a column with old indices
    strucmap.reset_index(inplace=True, drop=True)
    
    return(strucmap)


def url_sifts_strucmap(target_record, min_ping_time=5.0, verbose=0):
    target_id = target_record.id
    target_seq = str(target_record.seq)
    if verbose > 0:
        # print("")
        print("%s: Initial structure map from URL requests to SIFTS" % (target_id))
        # if verbose > 1:
        #     print("%s target sequence:" % (target_id))
        #     print(target_seq)
            
    # Get best structure mappings from sifts
    sifts_list = url_sifts(target_id, min_ping_time, verbose=verbose)
    n_struc = len(sifts_list)
    if verbose > 0:
        print("%s: Recieved map of %d structures from SIFTS" % (target_id, n_struc))
        if verbose > 2:
            print("%s: raw SIFTS record:" % (target_id))
            print(sifts_list)

    # SIFTS keys {'end': 101, 'chain_id': 'C', 'pdb_id': '6xog', 'start': 1, 'unp_end': 101, 'coverage': 1, 'unp_start': 1, 'resolution': 1.98,
    #             'experimental_method': 'X-ray diffraction', 'tax_id': 9606}
    columns = ['pdb', 'chain', 'score', 'select', 'method_exp', 'method_res', 'unp_start', 'unp_end', 'resseq_start', 'resseq_end', 'cover_exp']
    
    sifts_dict = {'pdb': [d['pdb_id'] for d in sifts_list],  'chain': [d['chain_id'] for d in sifts_list],
                  'score': [0.0]*n_struc, 'select': [1]*n_struc,
                  'method_exp': [d['experimental_method'] for d in sifts_list], 'method_res': [d['resolution'] for d in sifts_list],
                  'cover_exp': [d['coverage'] for d in sifts_list],
                  'unp_start': [d['unp_start'] for d in sifts_list], 'unp_end': [d['unp_end'] for d in sifts_list],
                  'resseq_start': [d['start'] for d in sifts_list], 'resseq_end': [d['end'] for d in sifts_list]}
    
    return( pd.DataFrame(sifts_dict, columns=columns) )
    
    
def align_strucmap(target_record, strucmap, min_ping_time=5.0, verbose=0):
    """Update a structure map with aligned RESSEQ from PDBe URL request"""

    target_id = target_record.id
    target_seq = str(target_record.seq)
    n_struc = len(strucmap.index)

    # New columns to add to strucmap
    cover_obs_col = []
    ident_exp_col = []
    ident_obs_col = []
    inserts_col = []
    deletions_col = []
    mismatch_col = []
    non_obs_col = []
    modified_col = []
    resseq_col = []
    aligned_resseq_col = []
    aligned_resseq_obs_col = []
        
    c = 0
    for smap in list(strucmap.itertuples(index=False))[c:]:
        c += 1
        if verbose > 0:
            print("=== Structure %d of %d for %s: %s-%s" % (c,n_struc,target_id,smap.pdb,smap.chain))
        
        # A structure map entry must have pdb and chain fields
        if not 'pdb' in smap._fields:
            raise ValueError("ERROR: Structure map row with no pdb field: %s" % (str(smap)))
        if not 'chain' in smap._fields:
            raise ValueError("ERROR: Structure map row with no chain field: %s" % (str(smap)))
        pdb_str = "%s-%s" % (smap.pdb, smap.chain)
        
        # Get RESSEQ sequence, i.e. the sequence of the protein actually used to determine structure (including residues not observed in the structure)
        try:
            pdb_resseq = url_pdb_resseq(smap.pdb, smap.chain, min_ping_time=min_ping_time, verbose=verbose)
        except Exception as err:
            print("ERROR: RESSEQ request for %s failed (%s)" % (pdb_str, err))
            cover_obs_col.append(np.nan)
            ident_exp_col.append(np.nan)
            ident_obs_col.append(np.nan)
            inserts_col.append(np.nan)
            deletions_col.append(np.nan)
            mismatch_col.append(np.nan)
            non_obs_col.append(np.nan)
            modified_col.append(np.nan)
            resseq_col.append(np.nan)
            aligned_resseq_col.append(np.nan)
            aligned_resseq_obs_col.append(np.nan)
            continue

        # Look for modified residues
        pdb_modified = url_pdb_modified(smap.pdb, smap.chain, min_ping_time=min_ping_time, verbose=verbose)
        
        # Request observed moities and residues in structure
        pdb_obs = url_pdb_observed(smap.pdb, chain_id=None, min_ping_time=min_ping_time, verbose=verbose)
        obs_chains = {}
        n_macro_mol = len(pdb_obs["molecules"])
        for i_macro_mol in range(n_macro_mol):
            # How many chains of macro molecule type (also has key 'entity_id' : integer)
            mmol_id = pdb_obs["molecules"][i_macro_mol]["entity_id"]
            n_mmol_chains = len(pdb_obs["molecules"][i_macro_mol]["chains"])
            for i_mmol_chain in range(n_mmol_chains):
                # ID of this chain (also has key 'struct_asym_id' : char)
                chain_id = pdb_obs["molecules"][i_macro_mol]["chains"][i_mmol_chain]["chain_id"]
                # How many fragments are observed for chain 
                n_chain_frag = len(pdb_obs["molecules"][i_macro_mol]["chains"][i_mmol_chain]["observed"])
                frag_list = []
                for i_frag in range(n_chain_frag):
                    start_dict = pdb_obs["molecules"][i_macro_mol]["chains"][i_mmol_chain]["observed"][i_frag]["start"]
                    end_dict = pdb_obs["molecules"][i_macro_mol]["chains"][i_mmol_chain]["observed"][i_frag]["end"]

                    # residue_number is 1-based RESSEQ index and author_residue_number is ATOM field numbers (both numbers observed)
                    frag_list.append((start_dict["residue_number"], end_dict["residue_number"], ))

                    # also has keys 'author_insertion_code' (typically None) and 'struct_asym_id' (typically chain id)
                    if not start_dict["author_insertion_code"] is None or not end_dict["author_insertion_code"] is None:
                        print("WARNING: PDB %s (%s) observed fragment start and end author_insertion_code is %s and %s" %
                              (pdb_str, target_id, start_dict["author_insertion_code"], end_dict["author_insertion_code"]))
                    # # Remove struct_asym_id warning, from 6n13-B it only seems to be different from chain_id when ordering is not A,B,C,...
                    # if start_dict["struct_asym_id"] != chain_id or end_dict["struct_asym_id"] != chain_id:
                    #     print("WARNING: PDB %s (%s) observed fragment start and end struct_asym_id %s and %s is different from chain_id %s" %
                    #           (pdb_str, target_id, start_dict["struct_asym_id"], end_dict["struct_asym_id"], chain_id))
                    
                if chain_id in obs_chains.keys():
                    print(pdb_obs)
                    raise ValueError("ERROR: Chain id %s present multiple times in PDB observed responce" % (chain_id))
                obs_chains[chain_id] = (mmol_id, frag_list)
                
        obs = obs_chains[smap.chain][1]
        nres = len(pdb_resseq)
        pdb_resseq_obs = pdb_resseq
        
        # Check for non-observed N-terminal
        if obs[0][0] > 1:
            pdb_resseq_obs = "x"*(obs[0][0]-1) + pdb_resseq_obs[(obs[0][0]-1):]
            if verbose > 0:
                print("%s: Non-observed region in %s: %d-%s-%d (1-based RESSEQ index)" %
                      (target_id, pdb_str, 1, pdb_resseq[0:(obs[0][0]-1)], obs[0][0]-1))
        # Non-observed regions
        if len(obs) > 1:
            for i in range(1,len(obs)):
                pdb_resseq_obs = pdb_resseq_obs[:obs[i-1][1]] + "x"*(obs[i][0]-obs[i-1][1]-1) + pdb_resseq_obs[(obs[i][0]-1):]
                if verbose > 0:
                    print("%s: Non-observed region in %s: %d-%s-%d (1-based RESSEQ index)" % (target_id, pdb_str, obs[i-1][1]+1,
                                                                                              pdb_resseq[(obs[i-1][1]):(obs[i][0]-1)], obs[i][0]-1))
        # Check for non-observed C-terminal
        if obs[-1][1] < nres:
            pdb_resseq_obs = pdb_resseq_obs[:obs[-1][1]] + "x"*(nres-obs[-1][1])
            if verbose > 0:
                print("%s: Non-observed region in %s: %d-%s-%d (1-based RESSEQ index)" %
                      (target_id, pdb_str, obs[-1][1]+1, pdb_resseq[obs[-1][1]:nres], nres))

        # Modified residues
        if len(pdb_modified) != 0:
            for pdbmod in pdb_modified:
                # This number should apply to the PDB RESSEQ sequence
                resi = pdbmod['residue_number']
                aa = pdb_resseq[resi-1]
                aa_obs = pdb_resseq_obs[resi-1]
                if aa != aa_obs:
                    if verbose > 0:
                        print("%s: Modified residues %s%d-%s seems unobserved: %s" % (pdb_str, aa, resi, pdbmod['chem_comp_id'], aa_obs))
                if verbose > 1:
                    print("%s has modification %s%d-%s into %s (author_resi %d may be uniprot)" %
                          (pdb_str, aa, resi, pdbmod['chem_comp_id'], pdbmod['chem_comp_name'], pdbmod['author_residue_number']))
                    # print("%s: %s entity_id %d, weight %.2f, alternate_conformers %s, author_insertion_code %s" %
                    #       (target_id, pdb_str, pdbmod['entity_id'], pdbmod['weight'], pdbmod['alternate_conformers'], pdbmod['author_insertion_code']))
                    # print("%s: %s modification description: %s" % (target_id, pdb_str, pdbmod['description']))
                    
                # Mark modified with capital X which should work in bolsum and I can still see what is gap and what is modified
                pdb_resseq_obs = pdb_resseq_obs[:(resi-1)] + 'X' + pdb_resseq_obs[resi:]
        
        # Attempt SIFT alignment if present
        identities = 0
        inserts = 0
        pdb_nres = 0.1
        if 'resseq_start' in smap._fields and 'unp_start' in smap._fields:
            pdb_nres = smap.resseq_end - smap.resseq_start + 1
            unp_nres = smap.unp_end - smap.unp_start + 1
            
            # Align PDB RESSEQ sequence to Uniprot sequence
            if pdb_nres == unp_nres:
                # Attempt align PDB to target sequence based on SIFTS
                aligned_resseq = "-"*(smap.unp_start-1) + pdb_resseq[(smap.resseq_start-1):(smap.resseq_end)] + "-"*(len(target_seq)-smap.unp_end)
                identities = sum( [aa1==aa2 and not aa2.lower() in "-xX" for aa1,aa2 in zip(target_seq.lower(),aligned_resseq.lower())] )
                aligned_resseq_obs = "-"*(smap.unp_start-1) + pdb_resseq_obs[(smap.resseq_start-1):(smap.resseq_end)] + \
                    "-"*(len(target_seq)-smap.unp_end)
                if verbose > 0:
                    if identities/pdb_nres < 0.9:
                        print("%s: poor SIFTS alignment of PDB %s with only %d of %d identical residues - attempt custom alignment" %
                              (target_id, pdb_str, identities, pdb_nres))
                    else:
                        print("%s: using SIFTS alignment for %s" % (target_id,pdb_str))
            else:
                if verbose > 0:
                    print("%s: SIFTS residue range for %s have different length for PDB (nres %d) and Target (nres %d) - attempt custom alignment" %
                          (target_id, pdb_str, pdb_nres, unp_nres))
                
        # If SIFTS alignment failed run pairwise alignment
        if identities/pdb_nres < 0.9:
            (aligned_resseq,aligned_resseq_obs,inserts) = subalign(target_record, SeqRecord(Seq(pdb_resseq), id=pdb_str),
                                                                   extra_seq=pdb_resseq_obs, verbose=verbose)
            
            # Count identities among residues present in the experiment (not necessarily observed)
            identities = sum( [aa1==aa2 and not aa2 in "-xX" for aa1,aa2 in zip(target_seq.lower(),aligned_resseq.lower())] )

        # Check if identity is within threshold
        if len(aligned_resseq) != len(target_seq):
            print(smap)
            print(target_seq)
            print(aligned_resseq)
            raise ValueError("ERROR: %s %s alignment has length %d different from target sequence legnth %d" %
                             (target_id, pdb_str, len(aligned_resseq), len(target_seq)))

        if identities/pdb_nres < 0.9:
            # Warn if PDB fragment has less than 90% identity
            print("WARNING %s: Poor custom alignment of PDB %s with only %d of %d identical residues - attempt custom alignment" %
                  (target_id, pdb_str, identities, pdb_nres))

        # Store stats and alignemnts in structure map, these should be extracted from aligned_resseq_obs or tested of consistance with this
        # Number of inserted positions in PDB (should be removed from aligned sequence)
        inserts_col.append(inserts)
        
        # Number of non-terminal positions deleted/missing in the experiment
        deletions_col.append(aligned_resseq_obs.strip('-').count('-'))

        # Check consistant calculation of SIFTS coverage 
        cover_pos = len(aligned_resseq_obs.strip('-')) - deletions_col[-1]
        cover_exp = cover_pos * 1.0/len(target_seq)
        # Coverage should be the fraction of target positions that the structure covers including indels (uncertainty from 2-digit rounding)
        if cover_exp - smap.cover_exp > 1e-3:   #0.0051:
            print("WARNING: %s %s re-calculated SIFTS coverage %.4f differ from cover_exp of %.4f (%d pos)" %
                  (target_id, pdb_str, cover_exp, smap.cover_exp, cover_pos))

        # Coverage of observed residues, i.e. residues that are not deleted or unobserved.
        # Mismatch and and observed modified are included in cover_obs
        cover_obs_pos = len(aligned_resseq_obs) - aligned_resseq_obs.count('-') - aligned_resseq_obs.count('x') # - aligned_resseq_obs.count('X')
        cover_obs = cover_obs_pos * 1.0/len(target_seq)
        cover_obs_col.append(cover_obs)
        
        # Check indels for consistancy with SIFTS coverage ranges
        # if pdb_nres - sifts['deletions'] + sifts['inserts'] - unp_nres > 0:
        if pdb_nres - deletions_col[-1] + inserts - unp_nres > 0:
            print("WARNING: %s %s SIFTS target range %d-%d (%d) and PDB range %d-%d (%d) is not consistant with %d deletions and %d insertions" %
                  (target_id, pdb_str, smap.unp_start, smap.unp_end, unp_nres,
                   smap.resseq_start, smap.resseq_end, pdb_nres, deletions_col[-1], inserts))
        
        # How many of the covered target positions are identical
        ident_exp_col.append(identities)
        # For consistancy with cover_obs, a modified residue is counted as identical
        # ident_obs_col.append( sum( [aa1==aa2 and not aa2.lower() in "-xX" for aa1,aa2 in zip(target_seq.lower(),aligned_resseq_obs.lower())] ) )
        ident_obs_col.append( sum( [aa1==aa2 and not aa2.lower() in "-x" for aa1,aa2 in zip(target_seq.lower(),aligned_resseq_obs.lower())] ) )
        
        # How many covered and observe positions does not match
        mismatch_col.append(sum([aa1!=aa2 and aa2 in "acdefghiklmnpqrstvwy" for aa1,aa2 in zip(target_seq.lower(),aligned_resseq_obs.lower())]))
        
        # How many og the covered residues are not observed
        non_obs_col.append(aligned_resseq_obs.count('x'))
        
        # How many og the covered residues are not observed
        modified_col.append(aligned_resseq_obs.count('X'))
        
        # # Removed, makes many warnings. 
        # if not sifts['non-obs'] == pdb_resseq_obs.lower().count('x'):
        #     print("WARNING: %s %s has %d non-observed positions that was never in the experiment (missing)" %
        #           (target_id, pdb_str, pdb_resseq_obs.lower().count('x')-sifts['non-obs']))

        resseq_col.append(pdb_resseq)
        aligned_resseq_col.append(aligned_resseq)
        aligned_resseq_obs_col.append(aligned_resseq_obs)
        
        # Report alignment
        print("%s: %s alignment: ident_exp %4d   ident_obs %4d  cover_exp %5.3f  cover_obs %5.3f  deletions %d  inserts %d  mismatch %d" %
              (target_id, pdb_str, ident_exp_col[-1], ident_obs_col[-1], smap.cover_exp, cover_obs_col[-1],
                deletions_col[-1], inserts_col[-1], mismatch_col[-1]))
        if verbose > 0:
            print("%s: Original %s RESSEQ:" % (target_id,pdb_str))
            print(pdb_resseq)
            print("%s: Aligned %s sequence:" % (target_id,pdb_str))
            print(aligned_resseq)
            print("%s: Aligned observed %s sequence (-: gap/deletion, x: unobserved (but present in experiment), X: modified):" % (target_id,pdb_str))
            print(aligned_resseq_obs)

    # Add columns to structure map and return updated strucmap
    strucmap['cover_obs'] = cover_obs_col
    strucmap['ident_exp'] = ident_exp_col
    strucmap['ident_obs'] = ident_obs_col
    strucmap['inserts'] = inserts_col
    strucmap['deletions'] = deletions_col
    strucmap['mismatch'] = mismatch_col
    strucmap['non-obs'] = non_obs_col
    strucmap['modified'] = modified_col
    strucmap['resseq'] = resseq_col
    strucmap['aligned_resseq'] = aligned_resseq_col
    strucmap['aligned_resseq_obs'] = aligned_resseq_obs_col
    return(strucmap)


def url_sifts(uniprot_id, min_ping_time=50, verbose=0):
    # Go easy on servers and wait if necessary
    global __sifts_time
    time_since_last = time.time() - __sifts_time
    if time_since_last < min_ping_time:
        if verbose > 0:
            print("Avoid frequent SIFTS requests and wait %.1f seconds" % (min_ping_time - time_since_last))
        time.sleep(min_ping_time - time_since_last)
    __sifts_time = time.time()
        
    request_url = "https://www.ebi.ac.uk/pdbe/api/mappings/best_structures/%s" % (uniprot_id)
    if verbose > 1:
        print("Request EBI SIFTS URL: %s" %request_url )
    responce = urllib.request.urlopen(request_url)
    # urllib does not decode, but this is rarely important
    json_str = urllib.parse.unquote( responce.read().decode('utf-8') )
    # Return a list of dicts with info on each PDB
    return( json.loads(json_str)[uniprot_id] )
    

def url_uniprot(uniprot_id, min_ping_time=50, verbose=0):
    """
    Returns a BioPython SeqRecord object with attributes seq, name, description and dbxrefs
    """
    # Go easy on servers and wait if necessary
    global __uniprot_time
    time_since_last = time.time() - __uniprot_time
    if time_since_last < min_ping_time:
        if verbose > 0:
            print("Avoid frequent Uniprot requests and wait %.1f seconds" % (min_ping_time - time_since_last))
        time.sleep(min_ping_time - time_since_last)
    __uniprot_time = time.time()
    
    request_url = "https://www.uniprot.org/uniprot/%s.xml" % (uniprot_id)
    if verbose > 1:
        print("Request uniprot URL: %s" %request_url )
    responce = urllib.request.urlopen(request_url)
    # if responce.status != 200:
    #     raise InternetError()
    # Return a biopython uniprot record object
    return( SeqIO.read(responce, "uniprot-xml") )


def url_pdb_resseq(pdb_id, chain_id=None, min_ping_time=50, verbose=0):
    # Go easy on servers and wait if necessary
    global __pdb_time
    time_since_last = time.time() - __pdb_time
    if time_since_last < min_ping_time:
        if verbose > 0:
            print("Avoid frequent PDB requests and wait %.1f seconds" % (min_ping_time - time_since_last))
        time.sleep(min_ping_time - time_since_last)
    __pdb_time = time.time()
    
    # chain_id is case sensitive, pdb_id is not
    request_url = "https://www.ebi.ac.uk/pdbe/entry/pdb/%s/fasta" % (pdb_id)
    if verbose > 1:
        print("Request PDB URL: %s" %request_url )
    responce = urllib.request.urlopen(request_url)
    fasta_str = urllib.parse.unquote( responce.read().decode('utf-8') )
    seq_records = SeqIO.parse(StringIO(fasta_str), "fasta")

    # Dict of seq per chain_id
    seq_index_dict = {}
    seq_list = []
    for sr in seq_records:
        descrip_list = sr.description.split("|")
        if descrip_list[0] != "pdb":
            raise ValueError("ERROR: %s RESSEQ responce description does not start with \'pdb|\'" % (pdb_id))
        if descrip_list[1] != pdb_id.lower():
            raise ValueError("ERROR: %s RESSEQ responce description does not start with \'pdb|%s|\'" % (pdb_id, pdb_id.lower()))
        
        for c in descrip_list[2].split():
            seq_index_dict[c] = len(seq_list)
        seq_list.append(str(sr.seq))
    # Change this when I know what to return as default, use first sequence for now
    if chain_id is None:
        return(seq_list[0])
    else:
        if not chain_id in seq_index_dict.keys():
            raise ValueError("Error: Cannot get RESSEQ, no chain %s in PDB %s" % (chain_id,pdb_id))
        return(seq_list[seq_index_dict[chain_id]])


def url_pdb_observed(pdb_id, chain_id=None, min_ping_time=50, verbose=0):
    # Example of returnced list element
    # https://www.ebi.ac.uk/pdbe/api/pdb/entry/polymer_coverage/6uyz

    # Apparantly, observed says nothing about why the non-observed positions are not observed, e.g if they were never in
    # the experiment or just not observed
    
    # Go easy on servers and wait if necessary
    global __pdb_time
    time_since_last = time.time() - __pdb_time
    if time_since_last < min_ping_time:
        if verbose > 0:
            print("Avoid frequent PDB requests and wait %.1f seconds" % (min_ping_time - time_since_last))
        time.sleep(min_ping_time - time_since_last)
    __pdb_time = time.time()

    # chain_id is case sensitive, pdb_id is not
    if chain_id is None:
        request_url = "https://www.ebi.ac.uk/pdbe/api/pdb/entry/polymer_coverage/%s" % (pdb_id)
    else:
        request_url = "https://www.ebi.ac.uk/pdbe/api/pdb/entry/polymer_coverage/%s/chain/%s" % (pdb_id,chain_id)
    if verbose > 1:
        print("Request PDB URL: %s" %request_url )
    responce = urllib.request.urlopen(request_url)
    json_str = urllib.parse.unquote( responce.read().decode('utf-8') )
    # Return a dicts with info on each PDB
    # if chain is given fragments are in dict['molecules'][0]['chains'][0]['observed']
    return( json.loads(json_str)[pdb_id.lower()] )
    

def url_pdb_modified(pdb_id, chain_id=None, min_ping_time=50, verbose=0):
    """URL lookup of modified residues in PDB entry"""

    # Example of returned list element
    # {'chem_comp_name': 'N(6)-ACETYLLYSINE', 'entity_id': 1, 'description': 'Small ubiquitin-related modifier 1', 'weight': 9567.801,
    # 'residue_number': 32, 'author_residue_number': 46, 'chain_id': 'A', 'alternate_conformers': 0, 'author_insertion_code': '',
    # 'chem_comp_id': 'ALY', 'struct_asym_id': 'A'}
    
    # Go easy on servers and wait if necessary
    global __pdb_time
    time_since_last = time.time() - __pdb_time
    if time_since_last < min_ping_time:
        if verbose > 0:
            print("Avoid frequent PDB requests and wait %.1f seconds" % (min_ping_time - time_since_last))
        time.sleep(min_ping_time - time_since_last)
    __pdb_time = time.time()

    # chain_id is case sensitive, pdb_id is not
    request_url = "https://www.ebi.ac.uk/pdbe/api/pdb/entry/modified_AA_or_NA/%s" % (pdb_id)
    if verbose > 1:
        print("Request PDB URL: %s" %request_url )
    try:
        responce = urllib.request.urlopen(request_url)
    except urllib.error.HTTPError as e:
        if e.code == 404:
            # URL error 'page not found' means that there are no modified residues annotated for this PDB
            return({})
        else:
            # All other errors are raised normally
            raise urllib.error.HTTPError(e)
        
    json_str = urllib.parse.unquote( responce.read().decode('utf-8') )
    modified_dict = json.loads(json_str)
    if not pdb_id.lower() in modified_dict.keys():
        raise ValueError("ERROR: PDB modified responce does not seem to cover %s" % (pdb_id))
    modified_dict = modified_dict[pdb_id.lower()]
        
    if chain_id is None:
        return(modified_dict)
    
    modified_chain = {}
    for ent in modified_dict:
        if ent['chain_id'] != ent['struct_asym_id'] and verbose > 1:
            print("%s: modified residue record has chain_id %s but struct_asym_id %s, using chain_id" %
                  (pdb_id, ent['chain_id'], ent['struct_asym_id']))
    
        # Check if chain already has a record of residue modification 
        if ent['chain_id'] in modified_chain.keys():
            modified_chain[ent['chain_id']].append(ent)
        else:
            modified_chain[ent['chain_id']] = [ent]
            
    if chain_id in modified_chain.keys():
        return(modified_chain[chain_id])
    else:
        # No annotations for requested chain
        return({})


def subalign(target_seqrecord, query_seqrecord, extra_seq=None, verbose=0):
    """Align query sequence to target sequence and return aligned query sequence with gaps inserted

    Query sequence is assumed to be a subsequence of target and a warning is generated if inserts 
    are removed from the query

    """
    target_id = target_seqrecord.id
    target_seq = str(target_seqrecord.seq)
    target_nres = len(target_seq)
    query_id = query_seqrecord.id
    query_seq = str(query_seqrecord.seq)
    
    # Setup aligner
    aligner = Align.PairwiseAligner()
    aligner.mode = 'global'
    aligner.substitution_matrix = Align.substitution_matrices.load("BLOSUM62")
    aligner.open_gap_score = -4.0 # Default value for '*' mutation in BLOSUM62
    aligner.extend_gap_score = -1.0
    aligner.end_open_gap_score = -1.0 # end gap open is like extension since terminal may be viewed as a gap
    score_uncertainty = 1
    # print(aligner)
    # print(Align.substitution_matrices.load("BLOSUM62"))
    
    alignments = sorted( aligner.align(target_seq, query_seq) )
    if len(alignments) > 1:
        if alignments[0].score - alignments[1].score < score_uncertainty:
            print("WARNING: Second alignment of %s versus %s is within uncertainty %.1f" %
                  (target_id, query_id, score_uncertainty))
            print("%s: Alignment scores: %.1f and %.1f" % (target_id, alignments[0].score,alignments[1].score))

    if verbose > 1:
        print("%s: Best alignment of %d score %.1f:" % (target_id, len(alignments), alignments[0].score))
        print(alignments[0])
        if len(alignments) > 1 and verbose > 2:
            print("%s: Second best alignment of %d score %.1f:" % (target_id, len(alignments), alignments[1].score))
            print(alignments[1])
        
    # Get target and query alignment paths in Bio.Align.PairwiseAlignment tuple format
    a = alignments[0].aligned; ta = a[0]; qa = a[1]
    inserts = 0
    aligned_query_seq = ""
    if extra_seq is None:
        aligned_extra_seq = None
    else:
        aligned_extra_seq = ""
        
    # If there's a missing range before first target fragment, these are gaps in the query sequence
    aligned_query_seq += "-"*ta[0][0]
    if not extra_seq is None:
        aligned_extra_seq += "-"*ta[0][0]
  
    # Add first query fragment
    aligned_query_seq += query_seq[qa[0][0]:qa[0][1]]
    if not extra_seq is None:
        aligned_extra_seq += extra_seq[qa[0][0]:qa[0][1]]

    # Process following path fragments
    for i in range(len(qa)-1):
        # Check that end-position of query fragment i, is start position of query fragment i+1
        if qa[i][1] != qa[i+1][0]:
            inserts += qa[i+1][0]-qa[i][1]
            if verbose > 0:
                print("%s: alignment of %s will remove an insert of %d residues: %d-%s-%d" %
                      (target_id, query_id, qa[i+1][0]-qa[i][1], qa[i][1], query_seq[qa[i][1]:qa[i+1][0]], qa[i+1][0]))
            
        # If there's a missing range in target path, these are gaps in the query sequence
        aligned_query_seq += "-"*(ta[i+1][0]-ta[i][1])
        if not extra_seq is None:
            aligned_extra_seq += "-"*(ta[i+1][0]-ta[i][1])
        
        # Add next query fragment
        aligned_query_seq += query_seq[qa[i+1][0]:qa[i+1][1]]
        if not extra_seq is None:
            aligned_extra_seq += extra_seq[qa[i+1][0]:qa[i+1][1]]
        if verbose > 0 and ta[i][1] != ta[i+1][0]:
            print("%s: query sequence %s has a deletion from (and including) target position %d to %d (%d residues)" %
                  (target_id, query_id, ta[i][1], ta[i+1][0]+1, ta[i+1][0]-ta[i][1]))
        
    # If final target fragment is not last target position, add gaps untill it is
    aligned_query_seq += "-"*(target_nres-ta[0-1][1])
    if not extra_seq is None:
        aligned_extra_seq += "-"*(target_nres-ta[0-1][1])
            
    return((aligned_query_seq, aligned_extra_seq, inserts))


def strucmap_non_redundant(strucmap, verbose=0):
    # list of pdb-chain identifiers
    id_list = ["%s-%s" % (pdb,chain) for pdb,chain in zip(strucmap['pdb'], strucmap['chain'])]

    # make a list of unique pdb-chain identifiers
    (unq_id,unq_index,unq_counts) = np.unique(id_list, return_index=True, return_counts=True)

    remove_rows = []
    for unqi in np.where(unq_counts > 1)[0]:
        # find structure map index of this structure
        smapi = np.where(np.array(id_list) == unq_id[unqi])[0]
        
        # just checking...
        if len(smapi) != unq_counts[unqi]:
            raise ValueError("ERROR: %s found to have %d mappings but only %d indices found" % (unq_id[unqi], unq_counts[unqi], len(smapi)))
        
        col_nonid = []
        for coli in range(len(strucmap.columns)):
            # values in column of the rows with identical pdb,chain
            col_val = list( strucmap.iloc[smapi,coli] )
            # column is OK if values are identical and not nan
            col_ok = False
            if type(col_val[0]) is float:
                if np.isnan(col_val[0]):
                    # check that all elements are nan
                    col_ok = np.all(np.isnan(col_val))
                else:
                    # check that all values are within machine precision
                    col_ok = np.all(np.array(col_val)-col_val[0] < 1e-10)
            else:
                # compare all others as identities
                col_ok = np.all(np.array(col_val) == col_val[0]) or col_val[0] is np.nan
                
            # store column indices that are not identical 
            if not col_ok:
                col_nonid.append(coli)
                print("WARNING: Multiple maps of %s has different values in column \"%s\": %s" %
                      (unq_id[unqi], strucmap.columns[coli], str(col_val)))

        # raise error if columns are not OK
        if len(col_nonid) > 0:
            # print columns that are not identical together with pdb and chain id
            print("WARNING: Structure %s occurs %d times in structures map with non-identical mapping in column(s): %s:" %
                  (unq_id[unqi], unq_counts[unqi], ",".join(strucmap.columns[col_nonid])))
            print(strucmap.iloc[smapi, [0,1]+col_nonid])
            # raise ValueError("ERROR: Found %d identical %s structures with non-identical mapping in columns %s:" %
            #                  (unq_counts[unqi],unq_id[unqi],",".join(strucmap.columns[col_nonid])))
        elif verbose > 0:
            print("WARNING: Structure %s occurs %d times in structure map, will only keep first occurence" % (unq_id[unqi], unq_counts[unqi]))
            if verbose > 1:
                print(strucmap.iloc[smapi])
            
        # all entities except first occurence marked for deletion 
        remove_rows.extend(list(smapi[1:]))

    # remove marked rows and make new index (row labels)
    strucmap.drop(remove_rows, axis=0, inplace=True)
    strucmap.reset_index(inplace=True, drop=True)
    
    # return non-redundant structure map
    return(strucmap)


def parse_args():
    """
    Argument parser function
    """

    parser = ArgumentParser( description="" )

    # Initiating/setup command line arguments
    parser.add_argument( 'uniprot_ids',
        type=str,
        help="Input uniprot IDs. Several should be separated by ;"
        )
    
    args = parser.parse_args()

    args.uniprot_ids = args.uniprot_ids.split(';')

    return args

if __name__ == '__main__':
    """Find the best PDB structure for a Uniprot ID based on the SIFTS database 
    """
    # Verbose levels: 0: Print results only, 1: Print hvor result was obtained, 2: Debug information
    verbose = 2

    overwrite_data_files = False
    
    # data_dir = "/Users/kristofferjohansson/work/project_human_structome/struc_select/data"
    data_dir = "./data"
    # struc_dir = "/Users/kristofferjohansson/work/project_human_structome/struc_select/struc"
    
    # filename = '/Users/kristofferjohansson/work/project_human_structome/anders_uniprot_to_pdb/TorbensGenes.csv'
    # if not os.path.isdir(data_dir):
    #     try:
    #         os.mkdir(data_dir)
    #     except FileNotFoundError:
    #         print("ERROR: Could not create data directory (parent dir missing?): %s" % (data_dir))
    #         sys.exit(2)
            
    # Test directory excistance

    if overwrite_data_files:
        print("WARNING: Will overwrite all data in %s" % (data_dir))
        
    min_ping_time = 1.0
    # From https://www.ebi.ac.uk/pdbe/pdbe-rest-api
    # How many API calls am I allowed? At present we rely on user restraint and have not set any hard limits. But in
    # future, depending on load on our servers, we may want to implement rate limiting.


    # get user input arguments
    args = parse_args()


    # # Torbens genes
    # gene_list_df = pd.read_csv(filename)
    # uniprot_id_list = list(gene_list_df['Entry'])

    # Rosetta test set:    PTEN     CALM1     SUMO1     UBE2I      TPK1       P53     MAPK1      TPMT      GAL4      UBI4
    uniprot_id_list  = ['P60484', 'P0DP23', 'P63165', 'P63279', 'Q9H3S4', 'P04637', 'P28482', 'P51580', 'P04386', 'P0CG63']
    # what we used         1D5R    4DJC_A    1WYW_B      2GRO    3S4Y_A    6RZ3_A      2OJG      2BZG    3COQ_A    3OLM_D
    # Rosetta test set:   MAPK1      TPMT      GAL4      UBI4     BRCA1     ADRB2      HRas
    uniprot_id_list += ['P28482', 'P51580', 'P04386', 'P0CG63', 'P38398', 'P07550', 'P01112']
    # what we used         2OJG      2BZG    3COQ_A    3OLM_D    1JM7_A    3KJ6_A    4Q21_A
    
    # uniprot_id_list = ['P60484','P40692','O60260','P12931','P09651','P31483','P04035']
    # uniprot_id_list = ['P01112']

    uniprot_id_list = args.uniprot_ids

    if verbose > 0:
        print("Find structures for %d uniprot entities" % (len(uniprot_id_list)))
    
    for uniprot_id in uniprot_id_list:
        # Get information on target protein either from file or url request
        uniprot_filename = "uniprot_" + uniprot_id + ".xml"
        has_uniprot_file = os.path.isfile(data_dir + "/" + uniprot_filename)
        if not has_uniprot_file or overwrite_data_files:
            if verbose > 0:
                print("%s: Getting uniprot information from URL request" % (uniprot_id))
            uniprot_record = url_uniprot(uniprot_id, min_ping_time=min_ping_time, verbose=verbose)
            SeqIO.write(uniprot_record, data_dir + "/" + uniprot_filename, "seqxml")
        else:
            if verbose > 0:
                print("%s: Getting uniprot information from data file %s" % (uniprot_id, uniprot_filename))
            # BUG: SeqIO write and read in seqxml format seems to loose the name attribute but I could not find another
            #   format that worked with the name attribute. Closest is 'pir' format the puts the name attribute in the
            #   beginning of the description attribute but looses all dbxrefs.
            uniprot_record = list( SeqIO.parse(data_dir + "/" + uniprot_filename, "seqxml") )[0]

        nres_uniprot = len(uniprot_record.seq)
        print("%s: Name: %s, description: %s, residues: %s" %
              (uniprot_record.id, uniprot_record.name, uniprot_record.description, nres_uniprot))
        if verbose > 0:
            print(str(uniprot_record.seq))
            
        # Get structure map
        strucmap_filename = "strucmap_" + uniprot_id + ".csv"
        has_strucmap_file = os.path.isfile(data_dir + "/" + strucmap_filename)
        if not has_strucmap_file or overwrite_data_files:
            print("%s: Building structure map from URL requests" % (uniprot_id))

            # Initial structure map from SIFTS URL request
            try:
                strucmap = url_sifts_strucmap(uniprot_record, min_ping_time=min_ping_time, verbose=verbose)
            except Exception as err:
                print("ERROR: %s SIFTS request failed (%s) skip uniprot entry" % (uniprot_id, err))
                continue
            
            n_struc_raw = len(strucmap.index)
            if n_struc_raw < 1:
                print("ERROR: No structures assigned in SIFTS for %s, skip uniprot entry", uniprot_id)
                continue

            # Check if any PDB+chain id occurs multiple times
            strucmap = strucmap_non_redundant(strucmap, verbose=verbose)
            n_struc_nr = len(strucmap.index)
        
            # Require a minimum number of residues in the structure in order to apply Rosetta
            # For P53_P04637, min_res 30 made 6rz3 go up from rank 364 to 277. Further min_res 40 to rank 22
            min_res = 40
            l = list( np.where(strucmap['cover_exp'] * nres_uniprot >= min_res)[0] )
            if len(l) == 0:
                print("WARNING: No structures of %s has coverage %d or more. Continuing with poor structures")
            elif len(l) < n_struc_nr:
                strucmap = strucmap.iloc[l]
                # if verbose > 0:
                #     print("%s: Reducing structure map from %d to %d structures with %d or more residues covered (SIFT coverage)" %
                #           (uniprot_id, n_struc_nr, len(l), min_res))
                strucmap.reset_index(inplace=True, drop=True)
            else:
                if verbose > 0:
                    print("%s: No structures with coverage < %d found" % (uniprot_id,min_res))
            n_struc_minres = len(strucmap.index)
            
            # Shortlist structure map based on SIFTS assignments of experimental method and coverage
            n_short = 1000
            if n_struc_minres > n_short:
                if verbose > 0:
                    print("%s: Reducing list of structures from %d to top %d based on SIFTS assignments alone" %
                          (uniprot_id, n_struc_minres, n_short))
                strucmap = rank_structures(uniprot_record, strucmap, verbose=verbose)
                strucmap = strucmap.iloc[range(n_short)]
                strucmap.reset_index(inplace=True, drop=True)
            else:
                if verbose > 0:
                    print("%s: No shortlisting applied" % (uniprot_id))
            n_struc_sl = len(strucmap.index)

            # # Find a given PDB in the strucmap
            # i = np.where(strucmap['pdb']=="6rz3")[0][0]
            # print(i)
            # print(strucmap.iloc[range(max(0,i-20),i+10)])
                
            print("%s: Number of mapped structures: sifts_raw: %d  non-redundant: %d  min_res_%d: %d  shortlist: %d" %
                  (uniprot_id, n_struc_raw, n_struc_nr, min_res, n_struc_minres, n_struc_sl))
            
            # Update structure map with observation of residues based on PDBe URL requests
            try:
                strucmap = align_strucmap(uniprot_record, strucmap, min_ping_time=min_ping_time, verbose=verbose)
            except Exception as err:
                print("ERROR: %s PDB alignments failed (%s)" % (uniprot_id,err))
                continue

        else:
            print("%s: Getting structure map from data file %s" % (uniprot_id, strucmap_filename))
            # Read struncture map from disk
            strucmap = pd.read_csv(data_dir + "/" + strucmap_filename, sep=';')

        # Columns for pretty-printing
        pcol = ["pdb", "chain", "score", "select", "method_exp", "method_res", "cover_exp", "cover_obs", "ident_exp", "ident_obs",
                "inserts", "deletions", "mismatch", "non-obs", "modified"]

        # Maske jeg kan bruge shortlist her? Kald den "select_structures(uniprot_record, strucmap, 20, verbose=verbose))
        # Select structures based on the SIFTS mapping
        strucmap = rank_structures(uniprot_record, strucmap, verbose=verbose)
        print("%s: best structure %s-%s  method: %s  cover_obs: %.3f  deletions: %d  inserts: %d  mismatch: %d  modified: %d" %
              (uniprot_id, strucmap.loc[0,'pdb'], strucmap.loc[0,'chain'], strucmap.loc[0,'method_exp'], strucmap.loc[0,'cover_obs'],
               strucmap.loc[0,'deletions'], strucmap.loc[0,'inserts'], strucmap.loc[0,'mismatch'], strucmap.loc[0,'modified']))
        if verbose > 0:
            ntop = 20
            print("%s top %d structures" % (uniprot_id,ntop))
            if len(strucmap.index) < ntop:
                print(strucmap[pcol])
            else:
                print(strucmap[pcol].iloc[range(ntop)])

        # Write ranked structure map to disk
        strucmap.to_csv(data_dir + "/" + strucmap_filename, index=False, sep=';')

        # Structure with highest cover_obs and cover_exp (secondary score)
        if verbose > 0:
            i_max_cover_obs = strucmap.sort_values(by=['cover_obs','score'], ascending=False).index.values[0]
            i_max_cover_exp = strucmap.sort_values(by=['cover_exp','score'], ascending=False).index.values[0]
            print("%s best structure and structures with highest cover_obs and cover_exp" % (uniprot_id))
            print(strucmap[pcol].iloc[[0,i_max_cover_obs,i_max_cover_exp]])
            
        # # Determine the optimal number of structures that cover target protein
        # n_select = number_of_covering_structures(uniprot_record, strucmap, verbose=verbose)

        # Write aligned sequences to FASTA file
        seqrec_list = [SeqRecord(uniprot_record.seq, id=uniprot_record.id, description=uniprot_record.description)]
        seqrec_list += [SeqRecord(Seq(seq_str), id="%s-%s" % (pdb,c), description="score=%.3f cover_obs=%.3f" % (score,cover_obs))
                        for (seq_str,pdb,c,score,cover_obs) in
                        zip(strucmap['aligned_resseq_obs'], strucmap['pdb'], strucmap['chain'], strucmap['score'], strucmap['cover_obs'])]
        SeqIO.write(seqrec_list, data_dir + "/alignments_" + uniprot_id + ".fasta", "fasta")
        
        print("")
